{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151c4932-bb5d-43f3-a0b0-618b36ce59ac",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025770de-5a2b-4f88-8eb8-707e2f8fb8da",
   "metadata": {},
   "source": [
    "The parameter, p, in the formula below, allows for the creation of other distance metrics. Euclidean distance is represented by this formula when p is equal to two, and Manhattan distance is denoted with p equal to one.\n",
    "\n",
    "The KNN classifier shows Y as 0 or 1, while the KNN regression method predicts the quantitative value of Y, and can also be continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898097c7-ba9e-472b-9c12-3061e31974c9",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c0469-4581-45af-b80a-0c2f89890b8a",
   "metadata": {},
   "source": [
    "One can use cross-validation to select the optimal value of k for the k-NN algorithm, which helps improve its performance and prevent overfitting or underfitting. Cross-validation is also used to identify the outliers before applying the KNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b3bdc-a26f-4aef-9c79-9c93a9ac18d1",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f451f-c894-49c0-9096-589dbab2707a",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a popular machine learning technique used for classification and regression tasks. It relies on the idea that similar data points tend to have similar labels or values.\n",
    "\n",
    "During the training phase, the KNN algorithm stores the entire training dataset as a reference. When making predictions, it calculates the distance between the input data point and all the training examples, using a chosen distance metric such as Euclidean distance.\n",
    "\n",
    "Next, the algorithm identifies the K nearest neighbors to the input data point based on their distances. In the case of classification, the algorithm assigns the most common class label among the K neighbors as the predicted label for the input data point. For regression, it calculates the average or weighted average of the target values of the K neighbors to predict the value for the input data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329aad3-7b8f-416c-9fe2-a7bfcfdec54b",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7a71b-64c6-41cf-97b4-6ab5173f8ba0",
   "metadata": {},
   "source": [
    "The most important hyperparameter for KNN is the number of neighbors (n_neighbors). Test values between at least 1 and 21, perhaps just the odd numbers. It may also be interesting to test different distance metrics (metric) for choosing the composition of the neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76675e59-2d78-46ec-852f-8fc503471d91",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350dbd3-2b31-4c67-8280-475e57e419f1",
   "metadata": {},
   "source": [
    "The performance of the K-NN algorithm is influenced by three main factors:-\n",
    "The distance function or distance metric used to determine the nearest neighbors.\n",
    "The decision rule used to derive a classification from the K-nearest neighbors.\n",
    "The number of neighbors used to classify the new example.\n",
    "\n",
    "Feature Scaling. Feature scaling is a method used to normalize the range of independent variables or features of data. \n",
    "Batch Normalization.\n",
    "Mini-Batch Gradient Descent. \n",
    "Gradient Descent with Momentum. \n",
    "RMSProp Optimization. \n",
    "Adam Optimization. \n",
    "Learning Rate Decay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71904f6b-2d7e-468c-aba8-1b0f655d60bf",
   "metadata": {},
   "source": [
    "A disadvantage of the KNN algorithm is that it does not create a generalized separable model. There is no summary equations or trees that can be produced by the training process that can be quickly applied to new records. Instead, KNN simply uses the training data itself to perform prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
